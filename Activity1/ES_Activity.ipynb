{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing Tweets\n",
    "* In this activity, students should read about some important aspects of ElasticSearch, namely configuring mapping and settings of ES indices \n",
    "* Introducing custom analyzers in the settings segment of an ES index of tweets\n",
    "* prepare settings and mappings for an index\n",
    "* (bulk) inserting tweets using elasticsearch api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This command is used to install elasticsearch library.\n",
    "# Use it once and comment it out again\n",
    "# !pip install elasticsearch\n",
    "\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "from dateutil import parser\n",
    "import configparser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config.read('config.ini')\n",
    "\n",
    "es_host = config.get('elasticsearch', 'host')\n",
    "es_username = config.get('elasticsearch', 'username')\n",
    "es_password = config.get('elasticsearch', 'password')\n",
    "\n",
    "es = Elasticsearch(\n",
    "    [es_host],\n",
    "    basic_auth=(es_username, es_password),\n",
    "    verify_certs=False\n",
    ")\n",
    "\n",
    "#name of the created index\n",
    "index_name = \"tweets\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Custom analyzers in ES\n",
    "In ES, the textual elements of docuements can be analyzed before being indexed. This analysis process can be done by the default \"standard\" analyzer, or a developer can build his own custom analyzer in the setting segment.\n",
    "To have a better idea about what analyzer can do with text, try out the following analyzer configuration.\n",
    "\n",
    "__Example-1__\n",
    "```json\n",
    "GET /_analyze \n",
    "{\n",
    "  \"tokenizer\": \"whitespace\",\n",
    "  \"analyzer\": \"text_processing\",\n",
    "  \"filter\": [\"lowercase\", \"stop\"],\n",
    "  \"char_filter\": [\"html_strip\"],\n",
    "  \"text\": \"text to be analyzed. It contains <html></html>\"\n",
    "}\n",
    "```\n",
    "__Example-2__\n",
    "```json\n",
    "GET /_analyze \n",
    "{\n",
    "  \"tokenizer\": \"standard\",\n",
    "  \"filter\": [\n",
    "    \"lowercase\",\n",
    "    {\n",
    "      \"type\": \"ngram\",\n",
    "      \"min_gram\": 3,\n",
    "      \"max_gram\": 4\n",
    "    }\n",
    "  ],\n",
    "  \"analyzer\": \"text_processing\",\n",
    "  \"char_filter\": [\"html_strip\"],\n",
    "  \"text\": \"text to be analyzed. It contains <html></html>\"\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [1]  Answer each of the following questions:\n",
    "In the below cell, we build the settings and mappings of an index (tweets). This code snippet should be called once to create and configure the index.\n",
    "1) talk about the ngram_filter filter and the purpose of using it:\n",
    "    The ngram filter is useful for search for partial of word matching,\n",
    "    this happen from tokenize each tokem into the partial word from min and max ngram that specified in the setting,\n",
    "    to specify the boundaries word range.\n",
    "\n",
    "    The ngram_filter filter will tokenize each token into the substring based on the min and max scale specified.\n",
    "    Example of tokenization process in ngram_filter if min_gram is 3 and max_gram is 4\n",
    "    \"mohammad\"\n",
    "    (3 min_gram) \"moh,\" \"oha,\" \"ham,\" \"amm,\" \"mma,\" and \"mad\"\n",
    "    (4 max_gram) \"moha,\" \"oham,\" \"hamm,\" \"amma,\" and \"mmad\"\n",
    "\n",
    "2) The analyzer `text_processing` is used to analyze the `text` field of the tweet. Update the above cell to link this analyzer with the `text` field:\n",
    "    This will link the setting text field with analyze text_processing \n",
    "    \"text\": {\n",
    "        \"type\": \"text\",\n",
    "        \"analyzer\": \"text_processing\" \n",
    "    },\n",
    "\n",
    "3) describe the purpose of the `ignore_above` fields:\n",
    "    Ensure that field value contain this ignore_above property does not exceed the specified size and limit the size of the field \n",
    "    and will be truncated when add this value to the index or to the this field value with the size more than the specified size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'tweets'})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create index with settings and mapping\n",
    "\n",
    "# This test is done during development only. \n",
    "if es.indices.exists(index=index_name):\n",
    "    es.indices.delete(index=index_name)\n",
    "    \n",
    "\n",
    "# index settings\n",
    "configurations = {\n",
    "        \"settings\": {\n",
    "            \"analysis\": {\n",
    "                \"filter\": {\n",
    "                    \"ngram_filter\": {\n",
    "                        \"type\": \"ngram\",\n",
    "                        \"min_gram\": 3,\n",
    "                        \"max_gram\": 4\n",
    "                    }\n",
    "                },\n",
    "                \"analyzer\": {\n",
    "                    \"text_processing\": {\n",
    "                        \"type\": \"custom\",\n",
    "                        \"tokenizer\": \"standard\",\n",
    "                        \"filter\": [\n",
    "                            \"lowercase\",\n",
    "                            \"ngram_filter\"\n",
    "                        ]\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"date\": {\n",
    "                    \"type\": \"date\",\n",
    "                    \"fields\": {\n",
    "                        \"keyword\": {\n",
    "                            \"type\": \"keyword\",\n",
    "                            \"ignore_above\": 256\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"flag\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"fields\": {\n",
    "                        \"keyword\": {\n",
    "                            \"type\": \"keyword\",\n",
    "                            \"ignore_above\": 256\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"id\": {\n",
    "                    \"type\": \"keyword\",\n",
    "                    \"ignore_above\": 256\n",
    "                },\n",
    "                \"target\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"fields\": {\n",
    "                        \"keyword\": {\n",
    "                            \"type\": \"keyword\",\n",
    "                            \"ignore_above\": 256\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"text\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"analyzer\": \"text_processing\" \n",
    "                },\n",
    "                \"user\": {\n",
    "                    \"type\": \"text\",\n",
    "                    \"fields\": {\n",
    "                        \"keyword\": {\n",
    "                            \"type\": \"keyword\",\n",
    "                            \"ignore_above\": 256\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "# create index\n",
    "es.indices.create(index=index_name, ignore=400, body=configurations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [2] fixing an error wit the date field\n",
    "The below cell shows how to insert one cell into the created index. However, it throws an exception when trying to insert the date value. Investigate the reason and fix it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_index': 'tweets', '_id': '2193602064', '_version': 1, 'result': 'created', '_shards': {'total': 2, 'successful': 1, 'failed': 0}, '_seq_no': 0, '_primary_term': 1}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# inserting records\n",
    "\n",
    "# target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "# ids: The id of the tweet ( 2087)\n",
    "# date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "# flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "# user: the user that tweeted (robotickilldozr)\n",
    "# text: the text of the tweet (Lyx is cool)\n",
    "    \n",
    "\n",
    "tweet = {\n",
    "  \"target\": \"4\",\n",
    "  \"id\": \"2193602064\",\n",
    "  \"date\": parser.parse(\"Tue Jun 16 08:40:49 PDT 2009\"),\n",
    "  \"flag\": \"NO_QUERY\",\n",
    "  \"user\": \"tinydiamondz\",\n",
    "  \"text\": \"Happy 38th Birthday to my boo of alll time!!! Tupac Amaru Shakur\"\n",
    "}\n",
    "\n",
    "res = es.index(index=index_name, id=tweet['id'], body=tweet)\n",
    "\n",
    "\n",
    "\n",
    "print(res)\n",
    "\n",
    "# Now check http://localhost:9200/tweets/_mappings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [3] Bulk insertion\n",
    "\n",
    "    #1) download the twitter file and compress it using gzip command\n",
    "    #2) read the tweets one by one using the above code, then complete it towards performing bulk insertion of tweets (bulks of 10000 tweets)\n",
    "    \n",
    "https://www.kaggle.com/kazanova/sentiment140/data#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded (10000) tweets to index!\n",
      "Successfully loaded (10000) tweets to index!\n",
      "Successfully loaded (10000) tweets to index!\n",
      "\n",
      "Exceeded the maximum size specified 24000\n",
      "\n",
      "Successfully loaded (10000) tweets to index!\n"
     ]
    }
   ],
   "source": [
    "import csv, gzip, warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "file_path = \"d:\\\\docs-tfidf\\\\tweets_ds.csv.gz\"\n",
    "\n",
    "batch_size = 10000\n",
    "actions = []\n",
    "\n",
    "with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "    csv_reader = csv.DictReader(f)\n",
    "    x = 0\n",
    "    \n",
    "    for i, row in enumerate(csv_reader, start=1):\n",
    "        try:\n",
    "            action = {\n",
    "                \"_op_type\": \"index\",\n",
    "                \"_index\": index_name,\n",
    "                \"_id\": row[\"id\"],\n",
    "                \"_source\": {\n",
    "                    \"target\": row[\"target\"],\n",
    "                    \"id\": row[\"id\"],\n",
    "                    \"date\": parser.parse(row[\"date\"]),\n",
    "                    \"flag\": row[\"flag\"],\n",
    "                    \"user\": row[\"user\"],\n",
    "                    \"text\": row[\"text\"]\n",
    "                }\n",
    "            }\n",
    "\n",
    "            actions.append(action)\n",
    "\n",
    "            if i % batch_size == 0:\n",
    "                try:\n",
    "                    helpers.bulk(es, actions)\n",
    "                    print(f\"Successfully loaded ({len(actions)}) tweets to index!\")\n",
    "                    x += len(actions)\n",
    "                    actions = []\n",
    "\n",
    "                    if x >= 24000:\n",
    "                        print(\"\\nExceeded the maximum size specified 24000\\n\")\n",
    "                        break\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error while indexing batch: {e}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing row {i}: {e}\")\n",
    "\n",
    "if actions:\n",
    "    try:\n",
    "        helpers.bulk(es, actions)\n",
    "        print(f\"Successfully loaded ({len(actions)}) tweets to index!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while indexing remaining batch: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alexandraalice\n",
      "thespiderman86\n",
      "smartie91\n",
      "xoxkaylac\n",
      "kristinmarie521\n"
     ]
    }
   ],
   "source": [
    "# Create query to get users names send tweets between (15:00:00 and 16:00:00 time In 2009-04-18 date)\n",
    "#  and with (text match sad word) and limit the size to 5 elements\n",
    "\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"bool\": {\n",
    "            \"must\": [\n",
    "                {\n",
    "                \"range\": {\n",
    "                    \"date\": {\n",
    "                    \"gte\": \"2009-04-18T15:00:00\",\n",
    "                    \"lte\": \"2009-04-18T16:00:00\"\n",
    "                    }}\n",
    "                },\n",
    "                {\n",
    "                \"match\": {\n",
    "                    \"text\": \"sad\"\n",
    "                }}\n",
    "            ] }\n",
    "        },\n",
    "        \"size\": 5\n",
    "    }\n",
    "\n",
    "results = es.search(index=index_name, body=query)\n",
    "\n",
    "for hit in results[\"hits\"][\"hits\"]:\n",
    "    print(hit[\"_source\"][\"user\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
